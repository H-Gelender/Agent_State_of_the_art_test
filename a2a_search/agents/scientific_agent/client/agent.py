#!/usr/bin/env python
"""
agents.py

Ce fichier contient toute la logique de l'agent et du LLM.
Modifiez ce fichier pour changer le mod√®le, la temp√©rature, ou le type d'agent.
"""

import os
from contextlib import AsyncExitStack

from typing import List, Any, Tuple
from langchain_google_genai import ChatGoogleGenerativeAI
from langgraph.prebuilt import create_react_agent

from .mcp_utils import (
    read_config_json,
    get_mcp_servers_config,
    connect_to_all_mcp_servers,
)

class AgentWrapper:
    """
    Wrapper qui unifie l'interface de diff√©rents types d'agents.
    Permet de changer d'agent sans modifier __main__.py
    """
    
    def __init__(self, agent, agent_type: str = "react"):
        """
        Args:
            agent: L'agent sous-jacent (React, simple LLM, etc.)
            agent_type: Type d'agent ("react" ou "simple")
        """
        self.agent = agent
        self.agent_type = agent_type
    
    async def ainvoke(self, query: str) -> str:
        """
        Invoque l'agent et retourne une r√©ponse sous forme de string.
        G√®re automatiquement les diff√©rences entre les types d'agents.
        
        Args:
            query: La question de l'utilisateur
            
        Returns:
            str: La r√©ponse de l'agent
        """
        try:
            if self.agent_type == "react":
                # Format pour React agent
                response = await self.agent.ainvoke({"messages": [("user", query)]})
                if "messages" in response:
                    last_message = response["messages"][-1]
                    if hasattr(last_message, 'content'):
                        return last_message.content
                    return str(last_message)
                return str(response)
            
            elif self.agent_type == "simple":
                # Format pour LLM simple
                response = await self.agent.ainvoke(query)
                if hasattr(response, 'content'):
                    return response.content
                return str(response)
            
            else:
                # Fallback g√©n√©rique
                response = await self.agent.ainvoke(query)
                if hasattr(response, 'content'):
                    return response.content
                return str(response)
                
        except Exception as e:
            return f"‚ùå Error in agent invocation: {e}"


def get_agent(tools: List[Any]) -> AgentWrapper:
    """
    Cr√©e et configure l'agent avec le LLM de votre choix.
    
    Modifiez cette fonction pour:
    - Changer le mod√®le (model="gemini-2.0-flash")
    - Ajuster la temp√©rature
    - Modifier les param√®tres du LLM
    - Utiliser un autre LLM (OpenAI, Anthropic, etc.)
    - Changer le type d'agent (React, simple, custom)
    
    Args:
        tools: Liste des outils MCP charg√©s
        
    Returns:
        AgentWrapper: Agent configur√© et pr√™t √† √™tre utilis√©
    """
    # Configuration du LLM - MODIFIEZ ICI
    api_key = os.getenv("GOOGLE_API_KEY")
    if not api_key:
        raise ValueError("‚ùå GOOGLE_API_KEY not found in environment variables.")
    
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.0-flash",  # Changez le mod√®le ici
        temperature=0,              # Ajustez la temp√©rature ici
        max_retries=2,              # Nombre de tentatives en cas d'erreur
        google_api_key=api_key
    )
    
    # CHOISISSEZ VOTRE TYPE D'AGENT ICI:
    
    # Option 1: Agent React (recommand√© pour utiliser les outils)
    if tools:
        agent = create_react_agent(llm, tools)
        agent_type = "react"
        print(f"\n‚úÖ React Agent created with {len(tools)} tools.")
    else:
        # Option 2: LLM simple (si pas d'outils)
        agent = llm
        agent_type = "simple"
        print(f"\n‚úÖ Simple LLM created (no tools).")
    
    # Option 3: Si vous voulez forcer un LLM simple m√™me avec des outils:
    # agent = llm.bind_tools(tools) if tools else llm
    # agent_type = "simple"
    # print(f"\n‚úÖ Simple LLM with tools created.")
    
    print(f"   Model: gemini-2.0-flash")
    print(f"   Temperature: 0")
    
    return AgentWrapper(agent, agent_type)

# --- NOUVELLE FONCTION CENTRALIS√âE ---
async def create_and_initialize_agent() -> Tuple[AgentWrapper, AsyncExitStack]:
    """
    La nouvelle "usine" √† agent. Lit la config, se connecte aux serveurs,
    cr√©e l'agent et le retourne pr√™t √† l'emploi.
    
    Returns:
        Un tuple contenant l'AgentWrapper initialis√© et l'AsyncExitStack
        pour g√©rer le cycle de vie des connexions.
    """
    print("üöÄ Initializing Agent...")
    config = read_config_json()
    mcp_servers = get_mcp_servers_config(config)
    
    if not mcp_servers:
        raise RuntimeError("‚ùå No MCP servers configured. Cannot start agent.")
        
    exit_stack = AsyncExitStack()
    try:
        tools = await connect_to_all_mcp_servers(mcp_servers, exit_stack, verbose=True)
        agent = get_agent(tools)
        # On retourne l'agent ET l'exit_stack pour que l'appelant g√®re la fermeture
        return agent, exit_stack
    except Exception as e:
        # En cas d'erreur, on s'assure de fermer le stack
        await exit_stack.aclose()
        raise e